{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "بِسْمِ اللهِ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom grid world environment ]\n",
    "class GridWorld:\n",
    "    def __init__(self,size):\n",
    "        self.size=size # size of the grid world\n",
    "        self.grid=np.zeros((size,size))\n",
    "        self.start_state=(0,0)\n",
    "        # placing the obstacle at the center of the grid world \n",
    "        # self.grid[size//2,size//2]=1\n",
    "        # self.obstacle_state=(size//2,size//2)\n",
    "        self.goal_state=(size-1,size-1)\n",
    "        self.current_state=self.start_state\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state=self.start_state\n",
    "        return self.current_state \n",
    "    \n",
    "    def step(self,action):\n",
    "        x_coord,y_coord =self.current_state \n",
    "        if action==0: # move up\n",
    "            x_coord=max(0,x_coord-1)\n",
    "        elif action==1: # move down \n",
    "            x_coord=min(self.size-1,x_coord+1)\n",
    "        elif action==2: # move left \n",
    "            y_coord=max(0,y_coord-1)\n",
    "        elif action==3: # move right \n",
    "            y_coord=min(self.size-1,y_coord+1)\n",
    "        else :\n",
    "            raise ValueError(\"Invalid Action\")\n",
    "        \n",
    "        # setting the current state to the updated coordinates \n",
    "        self.current_state=(x_coord,y_coord)\n",
    "\n",
    "        # if the current state is goal state , reward is 1 else 0\n",
    "        if self.current_state == self.goal_state:\n",
    "            reward=1\n",
    "        else :\n",
    "            reward=0\n",
    "        \n",
    "        if self.current_state==self.goal_state:\n",
    "            done = True\n",
    "        else :\n",
    "            done = False \n",
    "\n",
    "        return self.current_state,reward,done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-learning algorithm\n",
    "def q_learning(env,episodes,alpha,gamma,epsilon):\n",
    "    # timesteps for each episode \n",
    "    timesteps=100\n",
    "    # initializing the Q-table with random values (preferably zeros)\\\n",
    "    Q=np.zeros((env.size,env.size,4))\n",
    "    \n",
    "    # running each episode \n",
    "    for each_episode in range(episodes):\n",
    "        \n",
    "        # resetting the environment for each episode \n",
    "        state=env.reset()\n",
    "        # setting the done Flag to False \n",
    "        done=False \n",
    "        while not done:\n",
    "            # epsilon greedy action selection\n",
    "            # genereate a random number between 0 and 1\n",
    "            random=np.random.random()\n",
    "            if random<epsilon:\n",
    "                # exploration (random action)\n",
    "                action=np.random.randint(0,4)\n",
    "            elif random>=epsilon:\n",
    "                # exploitation\n",
    "                action=np.argmax(Q[state[0],state[1]])\n",
    "\n",
    "\n",
    "            # taking the action and observing the next state and reward and done flag\n",
    "            next_state,reward,done=env.step(action)\n",
    "\n",
    "            # updating the Q-table using Q-Rule \n",
    "            Q[state[0], state[1], action] += alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action])\n",
    "\n",
    "            state=next_state\n",
    "\n",
    "        if each_episode%timesteps==0:\n",
    "            print(f\"Episode {each_episode} completed\")\n",
    "\n",
    "        \n",
    "    return Q\n",
    "         \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the environment \n",
    "env=GridWorld(5)\n",
    "# defining the hyperparameters \n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "epsilon=0.1\n",
    "episodes=1000\n",
    "\n",
    "Q_table_final=q_learning(env,episodes,alpha,gamma,epsilon)\n",
    "print(Q_table_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
